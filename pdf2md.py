import logging
import time
import base64
import json
import yaml
import hashlib
import re
from pathlib import Path
from io import BytesIO
from concurrent.futures import ThreadPoolExecutor, as_completed
from pdf2image import convert_from_path
import pandas as pd
from PIL import Image
from openai import OpenAI
from docling_core.types.doc import PictureItem, TableItem
from docling.datamodel.pipeline_options import PdfPipelineOptions, RapidOcrOptions
from docling.document_converter import DocumentConverter, PdfFormatOption
from prompt.VLM_prompt import VLM_PROMPT
from prompt.text_type_prompt import TEXT_TYPE_PROMPT
from prompt.table_repair_prompt import TABLE_REPAIR_PROMPT
from prompt.text_repair_prompt import TEXT_REPAIR_PROMPT

# åŠ è½½é…ç½®æ–‡ä»¶
with open('config.yaml', 'r', encoding='utf-8') as f:
    config = yaml.safe_load(f)

logging.basicConfig(level=logging.INFO)
log = logging.getLogger(__name__)

# === é…ç½® === (ä»é…ç½®æ–‡ä»¶è¯»å–)
ENABLE_OCR = config['OCR']['enabled']
DASHSCOPE_API_KEY = config['VLM']['api_key']
VLM_API_URL = config['VLM']['base_url']
VLM_MODEL = config['VLM']['model']
TEXT_API_KEY = config['OPENAI']['api_key']
TEXT_API_URL = config['OPENAI']['base_url']
TEXT_MODEL = config['OPENAI']['model']
MAX_CONCURRENCY_VLM = config['VLM']['max_concurrency']
MAX_CONCURRENCY_TEXT = config['OPENAI']['max_concurrency']

input_pdf_path = Path("D:/Personal_Project/SmolDocling/pdfs/Gan.pdf")


# æ ¹æ® PDF æ–‡ä»¶è·¯å¾„ç”Ÿæˆå“ˆå¸Œå€¼
def generate_hash_from_file(file_path: Path) -> str:
    md5_hash = hashlib.md5()
    with file_path.open("rb") as f:
        for chunk in iter(lambda: f.read(4096), b""):
            md5_hash.update(chunk)
    return md5_hash.hexdigest()


# è·å–å“ˆå¸Œå€¼ä½œä¸ºå­ç›®å½•å
pdf_hash = generate_hash_from_file(input_pdf_path)
output_dir = Path.cwd() / "output" / pdf_hash
output_dir.mkdir(parents=True, exist_ok=True)
doc_filename = input_pdf_path.stem


# === è·å–PDFæ¯é¡µå¹¶ä¿å­˜ä¸ºå›¾ç‰‡ ===
def convert_pdf_to_images(pdf_path: Path, output_dir: Path):
    # ä»é…ç½®ä¸­è·å– Poppler è·¯å¾„
    poppler_path = Path(config['POPPLER']['path'])
    # åˆ›å»º page å­ç›®å½•
    page_dir = output_dir / "page"
    page_dir.mkdir(parents=True, exist_ok=True)
    # ä½¿ç”¨ pdf2image å°†æ¯ä¸€é¡µè½¬æ¢ä¸ºå›¾ç‰‡
    pages = convert_from_path(
        pdf_path,
        dpi=300,  # 300 DPI
        poppler_path=str(poppler_path)
    )
    for page_num, page in enumerate(pages, start=1):
        page_image_filename = page_dir / f"page-{page_num}.png"
        page.save(page_image_filename, 'PNG')
        log.info(f"ä¿å­˜ PDF ç¬¬ {page_num} é¡µï¼š{page_image_filename.resolve()}")


# === å›¾åƒ + Prompt â†’ Markdown è¡¨æ ¼ï¼ˆQwenï¼‰===
def ask_table_from_image(pil_image: Image.Image, prompt: str = TABLE_REPAIR_PROMPT) -> str:
    try:
        buffered = BytesIO()
        pil_image.save(buffered, format="JPEG")
        img_base64 = base64.b64encode(buffered.getvalue()).decode('utf-8')
        client = OpenAI(api_key=DASHSCOPE_API_KEY, base_url=VLM_API_URL)
        content = [
            {"type": "text", "text": prompt},
            {"type": "image_url", "image_url": {"url": f"data:image/jpeg;base64,{img_base64}"}}
        ]
        completion = client.chat.completions.create(
            model=VLM_MODEL,
            messages=[{"role": "user", "content": content}]
        )
        return completion.choices[0].message.content.strip()
    except Exception as e:
        log.warning(f"âŒ è¡¨æ ¼å›¾åƒä¿®å¤å¤±è´¥: {e}")
        return "[è¡¨æ ¼ä¿®å¤å¤±è´¥]"


# === å›¾ç‰‡æè¿° ===
def ask_image_vlm_base64(pil_image: Image.Image, prompt: str = VLM_PROMPT) -> str:
    try:
        buffered = BytesIO()
        pil_image.save(buffered, format="JPEG")
        img_base64 = base64.b64encode(buffered.getvalue()).decode('utf-8')
        client = OpenAI(api_key=DASHSCOPE_API_KEY, base_url=VLM_API_URL)
        content = [
            {"type": "text", "text": prompt},
            {"type": "image_url", "image_url": {"url": f"data:image/jpeg;base64,{img_base64}"}}
        ]
        completion = client.chat.completions.create(
            model=VLM_MODEL,
            messages=[{"role": "user", "content": content}]
        )
        return completion.choices[0].message.content.strip()
    except Exception as e:
        log.warning(f"å›¾åƒAPIå¤±è´¥: {e}")
        return "[å›¾åƒæè¿°å¤±è´¥]"

def needs_repair(text: str, threshold: int = 30) -> bool:
    # åŒ¹é…è¿ç»­çš„çº¯è‹±æ–‡å­—ç¬¦ä¸²ï¼ˆä¸åŒ…å«ä¸­æ–‡æˆ–ç©ºæ ¼ï¼‰
    matches = re.findall(r'[A-Za-z0-9,.\-()]{%d,}' % threshold, text)
    return len(matches) > 0


# å¤§æ¨¡å‹è¿›è¡Œè‹±æ–‡åˆ†è¯ä¿®å¤
def ask_repair_text(text: str) -> str:
    try:
        client = OpenAI(api_key=TEXT_API_KEY, base_url=TEXT_API_URL)
        prompt = f"{TEXT_REPAIR_PROMPT}\n{text}"
        response = client.chat.completions.create(
            model=TEXT_MODEL,
            messages=[{"role": "user", "content": prompt}]
        )
        repaired = response.choices[0].message.content.strip()
        return repaired
    except Exception as e:
        log.warning(f"âŒ è‹±æ–‡åˆ†è¯å¤±è´¥: {e}")
        return text  # å¤±è´¥æ—¶è¿”å›åŸæ–‡

# === åˆ¤æ–­æ–‡æœ¬ç±»å‹ï¼ˆæ ‡é¢˜ or æ­£æ–‡ï¼‰===
def ask_if_heading(text: str) -> str:
    try:
        client = OpenAI(api_key=TEXT_API_KEY, base_url=TEXT_API_URL)
        prompt = f"{TEXT_TYPE_PROMPT}\n{text}"
        response = client.chat.completions.create(
            model=TEXT_MODEL,
            messages=[{"role": "user", "content": prompt}]
        )
        answer = response.choices[0].message.content.strip().lower()
        return "heading" if "heading" in answer else "paragraph"
    except Exception as e:
        log.warning(f"åˆ¤æ–­æ ‡é¢˜/æ­£æ–‡å¤±è´¥: {e}")
        return "paragraph"

# === è¡¨æ ¼å›¾åƒåˆ‡å—å·¥å…·ï¼ˆæŒ‰å›ºå®šè¡Œé«˜è£åˆ‡ï¼‰ ===
def split_table_image_rows(pil_img: Image.Image, row_height: int = 400) -> list:
    width, height = pil_img.size
    slices = []
    for top in range(0, height, row_height):
        bottom = min(top + row_height, height)
        crop = pil_img.crop((0, top, width, bottom))
        slices.append(crop)
    return slices


# === æ‹¼æ¥ä¸ç¬¦åˆå°ºå¯¸é™åˆ¶çš„åˆ‡å— ===
def merge_small_chunks(chunks: list, min_height: int = 300, min_width: int = 20) -> list:
    merged_chunks = []
    temp_chunk = None

    for chunk in chunks:
        width, height = chunk.size

        # å¦‚æœå½“å‰å—å°ºå¯¸ä¸è¶³ï¼Œåˆ™å°è¯•æ‹¼æ¥ä¸Šä¸‹å—
        if height < min_height or width < min_width:
            if temp_chunk is None:
                temp_chunk = chunk
            else:
                # æ‹¼æ¥ä¸Šä¸‹å—
                new_chunk = Image.new("RGB", (max(temp_chunk.width, chunk.width), temp_chunk.height + chunk.height))
                new_chunk.paste(temp_chunk, (0, 0))
                new_chunk.paste(chunk, (0, temp_chunk.height))
                temp_chunk = new_chunk
        else:
            # å¦‚æœæœ‰æœªå¤„ç†çš„ä¸´æ—¶å—ï¼Œå…ˆä¿å­˜
            if temp_chunk is not None:
                merged_chunks.append(temp_chunk)
                temp_chunk = None
            merged_chunks.append(chunk)

    # æ·»åŠ æœ€åä¸€ä¸ªä¸´æ—¶å—ï¼ˆå¦‚æœæœ‰ï¼‰
    if temp_chunk is not None:
        # å¦‚æœæ•´ä¸ªè¡¨æ ¼å›¾ç‰‡çš„é«˜åº¦ä½äºæœ€å°é«˜åº¦ï¼Œåˆ™æŒ‰æœ€ä½é«˜åº¦è®¡ç®—
        if temp_chunk.height < min_height:
            new_chunk = Image.new("RGB", (temp_chunk.width, max(temp_chunk.height, 20)))
            new_chunk.paste(temp_chunk, (0, 0))
            merged_chunks.append(new_chunk)
        else:
            merged_chunks.append(temp_chunk)

    return merged_chunks


# === è·å–å…ƒç´ çš„è¾¹ç•Œæ¡† ===
def get_bbox(element):
    if hasattr(element, 'prov') and element.prov:
        bbox = element.prov[0].bbox
        return {
            "left": bbox.l,
            "top": bbox.t,
            "right": bbox.r,
            "bottom": bbox.b,
            "coord_origin": bbox.coord_origin
        }
    return None


# === ä¸»æµç¨‹ ===
def convert_pdf_to_markdown_with_images():
    start_time = time.time()
    pipeline_options = PdfPipelineOptions()
    pipeline_options.images_scale = 2.0
    pipeline_options.generate_picture_images = True
    pipeline_options.generate_table_images = True
    if ENABLE_OCR:
        pipeline_options.do_ocr = True
        pipeline_options.ocr_options = RapidOcrOptions(force_full_page_ocr=True)
    doc_converter = DocumentConverter(
        format_options={"pdf": PdfFormatOption(pipeline_options=pipeline_options)}
    )
    conv_res = doc_converter.convert(input_pdf_path)
    document = conv_res.document
    markdown_lines = []
    json_data = []
    table_counter = 0
    picture_counter = 0

    # è·å–å¹¶ä¿å­˜ PDF æ¯ä¸€é¡µä¸ºå›¾ç‰‡
    convert_pdf_to_images(input_pdf_path, output_dir)

    # å¹¶å‘ä»»åŠ¡é˜Ÿåˆ—
    vlm_executor = ThreadPoolExecutor(max_workers=MAX_CONCURRENCY_VLM)
    text_executor = ThreadPoolExecutor(max_workers=MAX_CONCURRENCY_TEXT)

    futures = []

    for element, level in document.iterate_items():
        # è·å–å…ƒç´ çš„è¾¹ç•Œæ¡†
        bbox = get_bbox(element)
        if isinstance(element, TableItem):
            table_counter += 1
            # ä½¿ç”¨å“ˆå¸Œå€¼ç”Ÿæˆå›¾ç‰‡/è¡¨æ ¼æ–‡ä»¶å
            table_image_filename = output_dir / f"{pdf_hash}-table-{table_counter}.png"
            pil_img = element.get_image(document)
            pil_img.save(table_image_filename, "PNG")
            table_df: pd.DataFrame = element.export_to_dataframe()
            if not table_df.columns.is_unique or table_df.shape[1] < 2:
                log.warning(f"âš ï¸ è¡¨æ ¼ {table_counter} ç»“æ„å¼‚å¸¸ï¼Œä½¿ç”¨ Qwen å¤šè½®å›¾åƒæ¨ç†ä¿®å¤")
                # è‡ªåŠ¨å›¾åƒåˆ‡å—
                sub_images = split_table_image_rows(pil_img)
                # æ‹¼æ¥ä¸ç¬¦åˆå°ºå¯¸é™åˆ¶çš„åˆ‡å—
                sub_images = merge_small_chunks(sub_images)

                # ğŸ” ä½¿ç”¨å±€éƒ¨å˜é‡ç®¡ç†è¯¥è¡¨æ ¼çš„ä¿®å¤ä»»åŠ¡
                chunk_futures = []
                for idx, chunk_img in enumerate(sub_images):
                    future = vlm_executor.submit(ask_table_from_image, chunk_img)
                    chunk_futures.append((future, idx, chunk_img))

                # æ”¶é›†ç»“æœ
                full_md_lines = []
                for future, idx, chunk_img in chunk_futures:
                    try:
                        chunk_md = future.result()
                        lines = chunk_md.splitlines()
                        if idx == 0:
                            full_md_lines.extend(lines)  # è¡¨å¤´ + åˆ†éš”çº¿
                        else:
                            full_md_lines.extend(lines[2:])  # ä»…æ•°æ®è¡Œ
                    except Exception as e:
                        log.warning(f"è¡¨æ ¼åˆ†å—å¤„ç†å¤±è´¥: {e}")

                markdown_lines.append(f"<!-- è¡¨æ ¼ {table_counter} ä½¿ç”¨ Qwen ä¿®å¤ï¼Œå·²åˆ†å—æ‹¼æ¥ -->")
                markdown_lines.append("\n".join(full_md_lines))
                markdown_lines.append("")
                json_data.append({
                    "type": "table",
                    "level": level,
                    "image": table_image_filename.name,
                    "source": "reconstructed_by_qwen_chunked",
                    "markdown": "\n".join(full_md_lines),
                    "page_number": element.prov[0].page_no,
                    "bbox": bbox
                })
                continue  # è·³è¿‡åŸå§‹å¤„ç†

            # âœ… è¡¨æ ¼ç»“æ„æ­£å¸¸
            markdown_lines.append(table_df.to_markdown(index=False))
            markdown_lines.append("")
            json_data.append({
                "type": "table",
                "level": level,
                "image": table_image_filename.name,
                "data": table_df.to_dict(orient="records"),
                "page_number": element.prov[0].page_no,
                "bbox": bbox
            })
        elif isinstance(element, PictureItem):
            picture_counter += 1
            # ä½¿ç”¨å“ˆå¸Œå€¼ç”Ÿæˆå›¾ç‰‡æ–‡ä»¶å
            picture_image_filename = output_dir / f"{pdf_hash}-picture-{picture_counter}.png"
            pil_img = element.get_image(document)
            pil_img.save(picture_image_filename, "PNG")
            future = vlm_executor.submit(ask_image_vlm_base64, pil_img)
            futures.append((future, "picture", {
                "image_path": picture_image_filename,
                "level": level,
                "page": element.prov[0].page_no,
                "bbox": bbox
            }))
        else:
            if hasattr(element, "text") and element.text:
                text = element.text.strip()
                if text:
                    if needs_repair(text):
                        log.info(f"å‘ç°å¼‚å¸¸æ— ç©ºæ ¼æ®µï¼Œè°ƒç”¨åˆ†è¯æ¨¡å‹ä¿®å¤: {text}")
                        text = ask_repair_text(text)

                    future = text_executor.submit(ask_if_heading, text)
                    futures.append((future, "text", {
                        "text": text,
                        "level": level,
                        "page": element.prov[0].page_no,
                        "bbox": bbox
                    }))

    # ç­‰å¾…æ‰€æœ‰å¹¶å‘ä»»åŠ¡å®Œæˆ
    for future, task_type, meta in futures:
        try:
            result = future.result()
            if task_type == "picture":
                caption = result
                markdown_lines.append(f"![{caption}](./{meta['image_path'].name})")
                json_data.append({
                    "type": "picture",
                    "level": meta["level"],
                    "image": meta["image_path"].name,
                    "caption": caption,
                    "page_number": meta["page"],
                    "bbox": meta["bbox"]
                })
            elif task_type == "text":
                label = result
                markdown_lines.append(f"# {meta['text']}" if label == "heading" else meta["text"])
                markdown_lines.append("")
                json_data.append({
                    "type": "text",
                    "level": meta["level"],
                    "text": meta["text"],
                    "label": label,
                    "page_number": meta["page"],
                    "bbox": meta["bbox"]
                })
        except Exception as e:
            log.warning(f"å¹¶å‘ä»»åŠ¡å¤±è´¥: {e}")

    # å…³é—­çº¿ç¨‹æ± 
    vlm_executor.shutdown(wait=True)
    text_executor.shutdown(wait=True)

    # ä¿å­˜ç»“æœ
    markdown_file = output_dir / f"{pdf_hash}.md"
    with markdown_file.open("w", encoding="utf-8") as f:
        f.write("\n".join(markdown_lines))
    json_file = output_dir / f"{pdf_hash}.json"
    with json_file.open("w", encoding="utf-8") as f:
        json.dump(json_data, f, indent=2, ensure_ascii=False)

    log.info(f"å®Œæˆ PDF è§£æï¼Œè€—æ—¶ {time.time() - start_time:.2f} ç§’")
    log.info(f"Markdown æ–‡ä»¶ï¼š{markdown_file.resolve()}")
    log.info(f"JSON æ–‡ä»¶ï¼š{json_file.resolve()}")


if __name__ == "__main__":
    convert_pdf_to_markdown_with_images()
